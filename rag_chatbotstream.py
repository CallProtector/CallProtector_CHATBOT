import os
import json
from dotenv import load_dotenv
from fastapi import FastAPI
from pydantic import BaseModel
from sse_starlette.sse import EventSourceResponse
from openai import OpenAI
from pinecone import Pinecone

# âœ… 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# âœ… 2. FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# âœ… 3. OpenAI & Pinecone ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index = pc.Index("legal-guideline")

# âœ… 4. ìš”ì²­ ëª¨ë¸ ì •ì˜
class Query(BaseModel):
    session_id: int
    question: str

# âœ… 5. ìœ ì‚¬ ë¬¸ë‹¨ ê²€ìƒ‰ (ë³¸ë¬¸+ë©”íƒ€ë°ì´í„° í¬í•¨)
def retrieve_context(query: str, top_k: int = 5):
    embedding = client.embeddings.create(
        input=[query],
        model="text-embedding-ada-002"
    ).data[0].embedding

    results = index.query(vector=embedding, top_k=top_k, include_metadata=True)

    context_blocks = []
    source_pages = []
    for match in results["matches"]:
        meta = match["metadata"]
        context_blocks.append(
            f"ğŸ“Œ ìœ í˜•: {meta.get('ìœ í˜•', 'ì—†ìŒ')}\n"
            f"ğŸ“– ë³¸ë¬¸: {meta.get('ë³¸ë¬¸', '')}\n"
            f"âš– ê´€ë ¨ ë²•ë¥ : {meta.get('ê´€ë ¨ ë²•ë¥ ', 'ì—†ìŒ')}\n"
            f"ğŸ“ ìš”ì•½: {meta.get('ìš”ì•½', '')}\n"
        )
        source_pages.append({
            "ìœ í˜•": meta.get("ìœ í˜•", "ì—†ìŒ"),
            "ê´€ë ¨ë²•ë¥ ": meta.get("ê´€ë ¨ ë²•ë¥ ", "ì—†ìŒ")
        })

    return "\n---\n".join(context_blocks), source_pages

# âœ… 6. GPT ìŠ¤íŠ¸ë¦¬ë° + JSON ì‘ë‹µ
@app.post("/stream")
async def stream_chat(query: Query):
    context, source_pages = retrieve_context(query.question)
    source_hint = json.dumps(source_pages, ensure_ascii=False, indent=2)

    prompt = f"""
ë„ˆëŠ” ì•…ì„±ë¯¼ì› ëŒ€ì‘ ë° ê´€ë ¨ ë²•ë¥  ìƒë‹´ì„ ë„ì™€ì£¼ëŠ” AIì•¼.
ì•„ë˜ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë°˜ë“œì‹œ ì½”ë“œ ë¸”ë¡ ì—†ì´ JSONë§Œ ì¶œë ¥í•´.
ë¬¸ì¥ì—ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œê¸€ ë„ì–´ì“°ê¸°ë¥¼ ìœ ì§€í•´.

ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
{{
  "answer": "ìì—°ìŠ¤ëŸ¬ìš´ í•œê¸€ ë‹µë³€ ë¬¸ì¥",
  "sourcePages": [
    {{
      "ìœ í˜•": "ìœ í˜•",
      "ê´€ë ¨ë²•ë¥ ": "ê´€ë ¨ ë²•ë¥ "
    }}
  ]
}}

### ì°¸ê³  ìë£Œ:
{context}

### ì§ˆë¬¸:
{query.question}
"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì•…ì„±ë¯¼ì› ëŒ€ì‘ ê°€ì´ë“œ ë° ê´€ë ¨ ë²•ë¥  ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒë‹´í•˜ëŠ” ì „ë¬¸ê°€ AIì•¼. ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆ."},
            {"role": "user", "content": prompt}
        ],
        stream=True
    )

    async def event_generator():
        full_response = ""
        # ğŸ”¹ ìŠ¤íŠ¸ë¦¬ë°: ì‹¤ì‹œê°„ UI í‘œì‹œ
        for chunk in response:
            delta = chunk.choices[0].delta.content
            if delta:
                full_response += delta
                yield f"data: {delta}\n\n"  # UI í‘œì‹œìš© (ì‹¤ì‹œê°„)
        
        # ğŸ”¹ ë§ˆì§€ë§‰ì—ë§Œ ì™„ì„±ëœ JSON ë³„ë„ ì „ì†¡ (íŒŒì‹± ìš©)
        yield f"data: [JSON]{full_response}\n\n"
        yield "data: [END]\n\n"

    return EventSourceResponse(event_generator())